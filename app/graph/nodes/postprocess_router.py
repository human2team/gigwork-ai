# app/graph/nodes/postprocess_router.py
import json
from app.graph.state import ChatState
from app.llm.openai_client import LLMClient

llm = LLMClient()

CLARIFICATION_SCHEMA = {
    "type": "object",
    "properties": {
        "question": {"type": "string"}
    },
    "required": ["question"]
}

def postprocess_router(state: ChatState) -> ChatState:
    """
    ì˜¤ë¥˜ê°€ ì¡´ì¬í•˜ë©´ -> error ë°°ì—´ì„ ê¸°ë°˜ìœ¼ë¡œ LLMìœ¼ë¡œ clarification ì§ˆë¬¸ ìƒì„±
    ì˜¤ë¥˜ê°€ ì—†ìœ¼ë©´ -> modeë¥¼ normalë¡œ ìœ ì§€í•˜ê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
    """

    if not state.error:
        state.mode = "normal"
        return state

    # ğŸ”¹ LLMì— ë„˜ê²¨ì¤„ ì—ëŸ¬ ì •ë³´ ì •ë¦¬ (JSON ë¬¸ìì—´ë¡œ ë³€í™˜)
    error_json = json.dumps(state.error, ensure_ascii=False, indent=2)

    system_prompt = """
ë‹¹ì‹ ì€ ì¡°ê±´ ê¸°ë°˜ ì•Œë°” ì±—ë´‡ì˜ Clarification ì§ˆë¬¸ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì¡°ê±´ ìˆ˜ì • ë˜ëŠ” ì¶”ì¶œ ë„ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ëª©ë¡ì…ë‹ˆë‹¤.
ê° ì˜¤ë¥˜ëŠ” condition_type, condition_value, operation_type, contentë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ì •í™•íˆ ì…ë ¥í•´ì•¼ í•˜ëŠ”ì§€, ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ë¥¼
ì•Œê¸° ì‰½ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì§ˆë¬¸ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
{
  "question": "..."
}
"""
    user_prompt = f"ì˜¤ë¥˜ ëª©ë¡:\n{error_json}\n\nì‚¬ìš©ìì—ê²Œ ë‹¤ì‹œ ì–´ë–¤ ë‚´ìš©ì„ ë¬¼ì–´ë³´ë©´ ì¢‹ì„ê¹Œìš”?"

    result = llm.chat_json(
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        schema=CLARIFICATION_SCHEMA,
        model="gpt-5-mini"
    )

    if result:
        state.pending_question = result["question"]
        state.response_text = result["question"]
        state.mode = "clarification"
    else:
        # LLM ì‹¤íŒ¨ ì‹œ fallback
        state.pending_question = "í™•ì‹¤í•˜ì§€ ì•Šì€ ë¶€ë¶„ì´ ìˆì–´ìš”. ë‹¤ì‹œ ì •í™•íˆ ë§ì”€í•´ ì£¼ì„¸ìš”."
        state.response_text = state.pending_question
        state.mode = "clarification"

    return state
